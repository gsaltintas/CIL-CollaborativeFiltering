{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic workflow for the CIL Project 1\n",
    "- Modules built on pytorch_lightning for easier interface and logging capabilities\n",
    "- Important functionality logged to Wandb (See [this](https://wandb.ai/site/articles/pytorch-lightning-with-weights-biases) article for more information)\n",
    "- NeuMF mostly implemented except for the $\\alpha$ weighting in the paper\n",
    "- Probably overfits because the models map everyhing to the same entry $\\to$\n",
    "    - [x] add weight initialization (Added on May 12)\n",
    "    - todo: check gradients (maybe batch norm or LeakyRELU or parametric RELU would work better)\n",
    "- Todo: Check the validation in the sample code and find an effective way to incorporate it into pytorch lightning\n",
    "\n",
    "## Some thoughts\n",
    "- Vanilla NCF (given version) gets ~>0.90 training loss, ~1.02 valid loss\n",
    "- GMF, MLP and Vanilla losses seem to be stuck around 1.26, to improve we may try\n",
    "    - Getting 5 outputs (for rating$=1, 2, \\dots, 5$)\n",
    "    - Normalize the ratings (but still a narrow range may not help that much)\n",
    "    - [x] Add dropout => trained with p=0.2, validaiton rmse ~1.6, 0.1 doesn't seem so good either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Comments\n",
    "- For the current version, cpu does a fair enough job\n",
    "- MLP network is kind of customized version of NeuralCF\n",
    "\n",
    "## PytorchLightning\n",
    "- Models are defined with [pytorchlightning](https://www.pytorchlightning.ai/) which makes training and logging a lot easier.\n",
    "- It automatically disables batchnorm and dropout for eval mode\n",
    "\n",
    "## Wandb\n",
    "- First time: add your WANDB_API_KEY to kaggle secrets by going to Add-ons -> Secrets\n",
    "- `wandb.watch(model)` logs gradients of the model\n",
    "- Model name is useful to log the losses with their names in wandb\n",
    "- Call `wandb.finish()` every time a new trianing starts otherwise checkpoints are overwritten.\n",
    "- Made my wandb project open to public, you can monitor the results [here](https://wandb.ai/gsaltintas/cil-project/overview?workspace=user-gsaltintas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:43:56.554066Z",
     "iopub.status.busy": "2022-05-13T14:43:56.553232Z",
     "iopub.status.idle": "2022-05-13T14:43:57.338315Z",
     "shell.execute_reply": "2022-05-13T14:43:57.337525Z",
     "shell.execute_reply.started": "2022-05-13T14:43:56.553997Z"
    }
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:55:33.634790Z",
     "iopub.status.busy": "2022-05-13T14:55:33.634354Z",
     "iopub.status.idle": "2022-05-13T14:55:44.070884Z",
     "shell.execute_reply": "2022-05-13T14:55:44.069183Z",
     "shell.execute_reply.started": "2022-05-13T14:55:33.634738Z"
    }
   },
   "outputs": [],
   "source": [
    "# If not running in your local machine, you should install pytorch_lightning and wandb at the beginning of each execution\n",
    "! pip install pytorch_lightning wandb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:09.608686Z",
     "iopub.status.busy": "2022-05-13T14:44:09.608311Z",
     "iopub.status.idle": "2022-05-13T14:44:14.543984Z",
     "shell.execute_reply": "2022-05-13T14:44:14.542750Z",
     "shell.execute_reply.started": "2022-05-13T14:44:09.608644Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:14.547480Z",
     "iopub.status.busy": "2022-05-13T14:44:14.546895Z",
     "iopub.status.idle": "2022-05-13T14:44:14.901292Z",
     "shell.execute_reply": "2022-05-13T14:44:14.900093Z",
     "shell.execute_reply.started": "2022-05-13T14:44:14.547428Z"
    }
   },
   "outputs": [],
   "source": [
    "# add your WANDB_API_KEY to kaggle secrets by going to Add-ons -> Secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\") \n",
    "# in a local computer add the following line to your bashrc or at the top of your notebook,\n",
    "# modify accordingly\n",
    "# os.environ['WANDB_API_KEY'] = 'WANDB_API_KEY'\n",
    "# you can also create wandb team and log the trainings there\n",
    "# gsaltintas/cil=project is now public so you can log there\n",
    "os.environ['WANDB_ENTITY'] = 'gsaltintas'\n",
    "# WANDB_API_KEY = os.environ['WANDB_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:14.903179Z",
     "iopub.status.busy": "2022-05-13T14:44:14.902846Z",
     "iopub.status.idle": "2022-05-13T14:44:17.492566Z",
     "shell.execute_reply": "2022-05-13T14:44:17.491402Z",
     "shell.execute_reply.started": "2022-05-13T14:44:14.903140Z"
    }
   },
   "outputs": [],
   "source": [
    "# this uses API key added to kaggle secrets\n",
    "! wandb login $WANDB_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:17.495092Z",
     "iopub.status.busy": "2022-05-13T14:44:17.494819Z",
     "iopub.status.idle": "2022-05-13T14:44:17.501758Z",
     "shell.execute_reply": "2022-05-13T14:44:17.500972Z",
     "shell.execute_reply.started": "2022-05-13T14:44:17.495052Z"
    }
   },
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# necessary for trainer later\n",
    "gpus = [0] if torch.cuda.is_available() else None\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Same as template code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:17.503163Z",
     "iopub.status.busy": "2022-05-13T14:44:17.502893Z",
     "iopub.status.idle": "2022-05-13T14:44:19.654069Z",
     "shell.execute_reply": "2022-05-13T14:44:19.653069Z",
     "shell.execute_reply.started": "2022-05-13T14:44:17.503132Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_users, number_of_movies = (10000, 1000)\n",
    "\n",
    "data_pd = pd.read_csv('/kaggle/input/cil-collaborative-filtering-2022/data_train.csv')\n",
    "test_pd = pd.read_csv('/kaggle/input/cil-collaborative-filtering-2022/sampleSubmission.csv')\n",
    "print(data_pd.head(5))\n",
    "print(test_pd.head(5))\n",
    "print()\n",
    "print('Shape', data_pd.shape, test_pd.shape, data_pd.shape[0]/test_pd.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:19.655625Z",
     "iopub.status.busy": "2022-05-13T14:44:19.655380Z",
     "iopub.status.idle": "2022-05-13T14:44:21.419547Z",
     "shell.execute_reply": "2022-05-13T14:44:21.417758Z",
     "shell.execute_reply.started": "2022-05-13T14:44:19.655596Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into train and test\n",
    "\n",
    "train_size = 0.9\n",
    "\n",
    "train_pd, test_pd = train_test_split(data_pd, train_size=train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:21.422480Z",
     "iopub.status.busy": "2022-05-13T14:44:21.422189Z",
     "iopub.status.idle": "2022-05-13T14:44:30.081875Z",
     "shell.execute_reply": "2022-05-13T14:44:30.080962Z",
     "shell.execute_reply.started": "2022-05-13T14:44:21.422449Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_users_items_predictions(data_pd):\n",
    "    users, movies = \\\n",
    "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
    "    predictions = data_pd.Prediction.values\n",
    "    return users, movies, predictions\n",
    "\n",
    "train_users, train_movies, train_predictions = extract_users_items_predictions(train_pd)\n",
    "\n",
    "# also create full matrix of observed values\n",
    "data = np.full((number_of_users, number_of_movies), np.mean(train_pd.Prediction.values))\n",
    "mask = np.zeros((number_of_users, number_of_movies)) # 0 -> unobserved value, 1->observed value\n",
    "\n",
    "for user, movie, pred in zip(train_users, train_movies, train_predictions):\n",
    "    data[user - 1][movie - 1] = pred\n",
    "    mask[user - 1][movie - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:30.083626Z",
     "iopub.status.busy": "2022-05-13T14:44:30.083293Z",
     "iopub.status.idle": "2022-05-13T14:44:30.093312Z",
     "shell.execute_reply": "2022-05-13T14:44:30.092143Z",
     "shell.execute_reply.started": "2022-05-13T14:44:30.083583Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:30.095101Z",
     "iopub.status.busy": "2022-05-13T14:44:30.094838Z",
     "iopub.status.idle": "2022-05-13T14:44:30.657613Z",
     "shell.execute_reply": "2022-05-13T14:44:30.656667Z",
     "shell.execute_reply.started": "2022-05-13T14:44:30.095068Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = lambda x, y: math.sqrt(mean_squared_error(x, y))\n",
    "\n",
    "test_users, test_movies, test_predictions = extract_users_items_predictions(test_pd)\n",
    "\n",
    "# test our predictions with the true values\n",
    "def get_score(predictions, target_values=test_predictions):\n",
    "    return rmse(predictions, target_values)\n",
    "\n",
    "def extract_prediction_from_full_matrix(reconstructed_matrix, users=test_users, movies=test_movies):\n",
    "    # returns predictions for the users-movies combinations specified based on a full m \\times n matrix\n",
    "    assert(len(users) == len(movies)), \"users-movies combinations specified should have equal length\"\n",
    "    predictions = np.zeros(len(test_users))\n",
    "\n",
    "    for i, (user, movie) in enumerate(zip(users, movies)):\n",
    "        predictions[i] = reconstructed_matrix[user][movie]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def mse_loss(predictions, target):\n",
    "    return torch.mean((predictions - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural CF\n",
    "Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:30.659856Z",
     "iopub.status.busy": "2022-05-13T14:44:30.659571Z",
     "iopub.status.idle": "2022-05-13T14:44:30.665430Z",
     "shell.execute_reply": "2022-05-13T14:44:30.664781Z",
     "shell.execute_reply.started": "2022-05-13T14:44:30.659823Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize with Gaussian distribution (0, 0.01)\n",
    "def initialize_weights(model, mean=0, std=0.01):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        nn.init.normal_(model.weight, mean, std)\n",
    "    if isinstance(model, nn.BatchNorm1d):\n",
    "        nn.init.normal_(model.weight, mean, std)\n",
    "        nn.init.constant_(model.bias, mean)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:05.256467Z",
     "iopub.status.busy": "2022-05-13T14:45:05.256112Z",
     "iopub.status.idle": "2022-05-13T14:45:05.280717Z",
     "shell.execute_reply": "2022-05-13T14:45:05.279446Z",
     "shell.execute_reply.started": "2022-05-13T14:45:05.256431Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseNCF(pl.LightningModule):\n",
    "    \"\"\" This is the BaseClass for underlying models, defines training and validation functionality, \n",
    "    and Adam optimizer\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size,                 \n",
    "                 lr: float = 0.001,\n",
    "                b1: float = 0.5,\n",
    "                b2: float = 0.999,\n",
    "                momentum: float = 0.9,\n",
    "                loss_func = mse_loss,\n",
    "                sublayer_name :str = ''):\n",
    "        \"\"\"\n",
    "        @param loss_func: specify the loss function for modularity, by default mse_loss\n",
    "        @param sublayer_name: useful for logging\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding_layer_users = nn.Embedding(number_of_users, embedding_size)\n",
    "        self.embedding_layer_movies = nn.Embedding(number_of_movies, embedding_size)\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * embedding_size, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=1), # maybe predict per category?\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, users, movies):\n",
    "        raise ('Must be implemented by subclasses')\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        users, movies, ratings = batch\n",
    "        predictions = self(users, movies)\n",
    "        loss = self.loss_func(predictions, ratings)\n",
    "        self.log(f'{self.hparams.sublayer_name}/train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        users, movies, ratings = batch\n",
    "        predictions = self(users, movies)\n",
    "        loss = self.loss_func(predictions, ratings)\n",
    "        self.log(f'{self.hparams.sublayer_name}/valid_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        weight_decay = 5*1e-4 \n",
    "        opt = torch.optim.Adam(self.parameters(), lr=lr, betas=(b1, b2), weight_decay= weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt,T_0=1, T_mult = 2,\n",
    "                                                             eta_min=1e-7)\n",
    "        return {\"optimizer\": opt,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\"\n",
    "                },\n",
    "               }\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        users, movies = batch\n",
    "        return self(users, movies)\n",
    "    \n",
    "    def name(self):\n",
    "#         todo: name model for saving purposes\n",
    "        return f\"BaseNCF_{'Adam'}\"\n",
    "\n",
    "class Vanilla_NCF(BaseNCF):\n",
    "    \"\"\" VanillaNCF class as implemented in the sample code \"\"\"\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size,                 \n",
    "                 sublayer_name='vanilla_ncf',\n",
    "                 *args, **kwargs):\n",
    "        super().__init__( number_of_users, number_of_movies, embedding_size, *args, **kwargs)\n",
    "        self.embedding_layer_users = nn.Embedding(number_of_users, embedding_size)\n",
    "        self.embedding_layer_movies = nn.Embedding(number_of_movies, embedding_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * embedding_size, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=1), # maybe predict per category?\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "        users_embedding = self.embedding_layer_users(users)\n",
    "        movies_embedding = self.embedding_layer_movies(movies)\n",
    "        concat = torch.cat([users_embedding, movies_embedding], dim=1)\n",
    "        return torch.squeeze(self.feed_forward(concat))\n",
    "    \n",
    "   \n",
    "    def name(self):\n",
    "        return f\"Vanilla_NCF_{'Adam'}\"\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        return {\"optimizer\": opt,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:08.908538Z",
     "iopub.status.busy": "2022-05-13T14:45:08.908202Z",
     "iopub.status.idle": "2022-05-13T14:45:08.916069Z",
     "shell.execute_reply": "2022-05-13T14:45:08.915235Z",
     "shell.execute_reply.started": "2022-05-13T14:45:08.908506Z"
    }
   },
   "outputs": [],
   "source": [
    "class GMF(BaseNCF):\n",
    "    \"\"\" Generalized Matrix Factoization network as described in the paper \"\"\"\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size,\n",
    "                 sublayer_name='gmf',\n",
    "                 *args, **kwargs):\n",
    "        super(GMF, self).__init__(number_of_users, number_of_movies, embedding_size,)\n",
    "        self.embedding_layer_users = nn.Embedding(number_of_users, embedding_size)\n",
    "        self.embedding_layer_movies = nn.Embedding(number_of_movies, embedding_size)\n",
    "        self.output_layer = nn.Linear(in_features=embedding_size, out_features=1)\n",
    "        \n",
    "    def forward(self, users, movies):\n",
    "        users_embedding = self.embedding_layer_users(users)\n",
    "        movies_embedding = self.embedding_layer_movies(movies)\n",
    "        return self.output_layer(users_embedding * movies_embedding)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:09.111949Z",
     "iopub.status.busy": "2022-05-13T14:45:09.111384Z",
     "iopub.status.idle": "2022-05-13T14:45:09.122432Z",
     "shell.execute_reply": "2022-05-13T14:45:09.121536Z",
     "shell.execute_reply.started": "2022-05-13T14:45:09.111913Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(BaseNCF):\n",
    "    \"\"\" MLP network of NeuMF modified from the paper (BatchNorm added, Leaky ReLU instead of RELU)\"\"\"\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size,\n",
    "                 layer_sizes=[64, 32, 16, 8], # may be used to define feed_forward layer in a more sophisticated way\n",
    "                 sublayer_name='mlp',\n",
    "                 dropout: float = 0,\n",
    "                 *args, **kwargs):\n",
    "        \"\"\"\n",
    "        dropout: dropout probability, by default 0\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__(number_of_users, number_of_movies, embedding_size,)\n",
    "        self.embedding_layer_users = nn.Embedding(number_of_users, embedding_size)\n",
    "        self.embedding_layer_movies = nn.Embedding(number_of_movies, embedding_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * embedding_size, out_features=64),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=64, out_features=16), \n",
    "            nn.BatchNorm1d(num_features=16),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=16, out_features=1), \n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, users, movies):\n",
    "        users_embedding = self.embedding_layer_users(users)\n",
    "        movies_embedding = self.embedding_layer_movies(movies)\n",
    "        concat = torch.cat([users_embedding, movies_embedding], dim=1)\n",
    "        hidden_layers = torch.squeeze(self.feed_forward(concat)) \n",
    "        return hidden_layers\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:09.611646Z",
     "iopub.status.busy": "2022-05-13T14:45:09.611341Z",
     "iopub.status.idle": "2022-05-13T14:45:09.622604Z",
     "shell.execute_reply": "2022-05-13T14:45:09.621644Z",
     "shell.execute_reply.started": "2022-05-13T14:45:09.611612Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalMLP(BaseNCF):\n",
    "    \"\"\" TODO: MLP network for multiclass classification  \"\"\"\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size,\n",
    "                 layer_sizes=[64, 32, 16, 8], # may be used to define feed_forward layer in a more sophisticated way\n",
    "                 sublayer_name='multimodal_mlp',\n",
    "                 *args, **kwargs):\n",
    "        super(MultiModalMLP, self).__init__(number_of_users, number_of_movies, embedding_size,)\n",
    "        self.embedding_layer_users = nn.Embedding(number_of_users, embedding_size)\n",
    "        self.embedding_layer_movies = nn.Embedding(number_of_movies, embedding_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(in_features=2 * embedding_size, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=32, out_features=16), # maybe predict per category?\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Linear(in_features=16, out_features=5), \n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "#         self.output_layer = nn.Linear(8, 1)\n",
    "        self.output_layer = nn.Linear(16, 5)\n",
    "        \n",
    "    def forward(self, users, movies):\n",
    "        users_embedding = self.embedding_layer_users(users)\n",
    "        movies_embedding = self.embedding_layer_movies(movies)\n",
    "        concat = torch.cat([users_embedding, movies_embedding], dim=1)\n",
    "        hidden_layers = torch.squeeze(self.feed_forward(concat)) \n",
    "        return self.output_layer(hidden_layers)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:09.956099Z",
     "iopub.status.busy": "2022-05-13T14:45:09.955819Z",
     "iopub.status.idle": "2022-05-13T14:45:09.967153Z",
     "shell.execute_reply": "2022-05-13T14:45:09.966181Z",
     "shell.execute_reply.started": "2022-05-13T14:45:09.956070Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(BaseNCF):\n",
    "    def __init__(self, number_of_users, number_of_movies, embedding_size,                 \n",
    "                 lr: float = 0.001,\n",
    "                 alpha = 0.5, # trade-off parameter between two models\n",
    "                 gmf_pretrained_ckpt = '',\n",
    "                 mlp_pretrained_ckpt = '',\n",
    "                 sublayer_name='neumf',\n",
    "                 *args, **kwargs):\n",
    "        super().__init__( number_of_users, number_of_movies, embedding_size,)\n",
    "        \n",
    "        self.gmf = GMF.load_from_checkpoint(gmf_pretrained_ckpt); self.gmf.mode = 'train'\n",
    "        self.mlp = MLP.load_from_checkpoint(mlp_pretrained_ckpt); self.mlp.mode = 'train'\n",
    "        self.output_layer = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, users, movies):\n",
    "        gmf = self.gmf(users, movies)\n",
    "        mlp = self.mlp(users, movies)\n",
    "        concat = torch.cat([gmf, mlp], dim=1)\n",
    "#         todo: implement \\alpha later\n",
    "        return self.output_layer(concat)\n",
    "    \n",
    "   \n",
    "    def name(self):\n",
    "        return f\"Full_NCF_{'VanillaSGD'}\"\n",
    "    \n",
    "     \n",
    "    # The paper uses VanillaSGD at this layer as they start the GMF and MLP layers pretrained\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        opt = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        return {\"optimizer\": opt}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining & Setup\n",
    "Pretrain GMF and MLP layers\n",
    "\n",
    "NOTE: In the sample code, they reform the matrix and calculate a different metric at the end of each epoch, we don't do that currently and only calculate validation loss on a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:11.358010Z",
     "iopub.status.busy": "2022-05-13T14:45:11.357448Z",
     "iopub.status.idle": "2022-05-13T14:45:11.362001Z",
     "shell.execute_reply": "2022-05-13T14:45:11.361323Z",
     "shell.execute_reply.started": "2022-05-13T14:45:11.357973Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 1024\n",
    "valid_batch_size = 1024\n",
    "num_epochs = 25\n",
    "show_validation_score_every_epochs = 1\n",
    "embedding_size = 16\n",
    "learning_rate = 1e-3\n",
    "pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:45:12.811014Z",
     "iopub.status.busy": "2022-05-13T14:45:12.810504Z",
     "iopub.status.idle": "2022-05-13T14:45:12.833637Z",
     "shell.execute_reply": "2022-05-13T14:45:12.832782Z",
     "shell.execute_reply.started": "2022-05-13T14:45:12.810979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Dataloaders\n",
    "train_users_torch = torch.tensor(train_users, device=device)\n",
    "train_movies_torch = torch.tensor(train_movies, device=device)\n",
    "train_predictions_torch = torch.tensor(train_predictions, device=device)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "#     todo: try normalizing for the inputs\n",
    "    TensorDataset(train_users_torch, train_movies_torch, train_predictions_torch),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_users_torch = torch.tensor(test_users, device=device)\n",
    "test_movies_torch = torch.tensor(test_movies, device=device)\n",
    "test_predictions_torch = torch.tensor(test_predictions, device=device)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TensorDataset(test_users_torch, test_movies_torch, test_predictions_torch),\n",
    "    batch_size=valid_batch_size)\n",
    "\n",
    "wandb_logger = WandbLogger(project='cil-project',\n",
    "                           log_model='all', \n",
    "                           )\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "# saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"valid_loss\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    ")\n",
    "trainer = pl.Trainer(callbacks=[checkpoint_callback, lr_monitor], \n",
    "                     gpus=gpus, auto_select_gpus=True,\n",
    "                     max_epochs=num_epochs, \n",
    "                     auto_lr_find=True,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=5,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:37.115592Z",
     "iopub.status.busy": "2022-05-13T14:44:37.114619Z",
     "iopub.status.idle": "2022-05-13T14:44:37.120991Z",
     "shell.execute_reply": "2022-05-13T14:44:37.120017Z",
     "shell.execute_reply.started": "2022-05-13T14:44:37.115534Z"
    }
   },
   "outputs": [],
   "source": [
    "# when implemented, multimodal mlp can be defined here\n",
    "# multi = MultiModalMLP(number_of_users, number_of_movies, embedding_size,\n",
    "#          loss_func=mse_loss).to(device)\n",
    "# initialize_weights(multi, 0, 0.01)\n",
    "\n",
    "# # wandb.finish()\n",
    "# wandb_logger = WandbLogger(project='cil-project',\n",
    "#                            log_model='all', \n",
    "#                            name=f'multi_{time.time():.0f}'\n",
    "#                            )\n",
    "# # uncomment to log gradients\n",
    "# wandb_logger.watch(multi)\n",
    "\n",
    "# checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "#     monitor=\"multimodal_mlp/valid_loss\",\n",
    "#     save_top_k=2,\n",
    "#     mode=\"min\",\n",
    "# )\n",
    "# trainer = pl.Trainer(callbacks=[checkpoint_callback, lr_monitor], \n",
    "# #                      gpus=list(range(torch.cuda.device_count())), \n",
    "#                      gpus=gpus, auto_select_gpus=True,\n",
    "#                      max_epochs=num_epochs, \n",
    "#                      auto_lr_find=True,\n",
    "#                      logger=wandb_logger,\n",
    "#                      log_every_n_steps=5,\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:44:37.421500Z",
     "iopub.status.busy": "2022-05-13T14:44:37.421049Z",
     "iopub.status.idle": "2022-05-13T14:44:37.425563Z",
     "shell.execute_reply": "2022-05-13T14:44:37.424685Z",
     "shell.execute_reply.started": "2022-05-13T14:44:37.421467Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainer.fit(multi, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VanillaNCF\n",
    "Uncomment to train VanillaNCF as presented in the sample notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T11:37:35.564288Z",
     "iopub.status.busy": "2022-05-13T11:37:35.563917Z",
     "iopub.status.idle": "2022-05-13T11:37:48.587763Z",
     "shell.execute_reply": "2022-05-13T11:37:48.586839Z",
     "shell.execute_reply.started": "2022-05-13T11:37:35.564244Z"
    }
   },
   "outputs": [],
   "source": [
    "ncf = Vanilla_NCF(number_of_users, number_of_movies, embedding_size*4,\n",
    "         loss_func=mse_loss).to(device)\n",
    "initialize_weights(ncf, 0, 0.01)\n",
    "\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project='cil-project',\n",
    "                           log_model='all', \n",
    "                           name=f'vanilla_ncf_{time.time():.0f}'\n",
    "                           )\n",
    "# uncomment to log gradients\n",
    "wandb_logger.watch(ncf)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"vanilla_ncf/valid_loss\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    ")\n",
    "trainer = pl.Trainer(callbacks=[checkpoint_callback, lr_monitor], \n",
    "                     gpus=gpus, auto_select_gpus=True,\n",
    "                     max_epochs=num_epochs, \n",
    "                     auto_lr_find=True,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=5,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T11:39:43.183414Z",
     "iopub.status.busy": "2022-05-13T11:39:43.183030Z",
     "iopub.status.idle": "2022-05-13T11:52:57.827463Z",
     "shell.execute_reply": "2022-05-13T11:52:57.826572Z",
     "shell.execute_reply.started": "2022-05-13T11:39:43.183373Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    trainer.fit(ncf, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define & Pretrain GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T09:35:31.240832Z",
     "iopub.status.busy": "2022-05-12T09:35:31.240576Z",
     "iopub.status.idle": "2022-05-12T09:36:01.627137Z",
     "shell.execute_reply": "2022-05-12T09:36:01.626547Z",
     "shell.execute_reply.started": "2022-05-12T09:35:31.240803Z"
    }
   },
   "outputs": [],
   "source": [
    "gmf = GMF(number_of_users, number_of_movies, embedding_size,\n",
    "         loss_func=mse_loss).to(device)\n",
    "initialize_weights(gmf, 0, 0.01)\n",
    "\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project='cil-project',\n",
    "                           log_model='all', \n",
    "                           name=f'gmf_{time.time():.0f}'\n",
    "                           )\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"gmf/valid_loss\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    ")\n",
    "trainer = pl.Trainer(callbacks=[checkpoint_callback, lr_monitor], \n",
    "                     gpus=gpus, auto_select_gpus=True,\n",
    "                     max_epochs=num_epochs, \n",
    "                     auto_lr_find=True,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=5,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T09:36:01.62908Z",
     "iopub.status.busy": "2022-05-12T09:36:01.628662Z",
     "iopub.status.idle": "2022-05-12T09:50:31.867834Z",
     "shell.execute_reply": "2022-05-12T09:50:31.866312Z",
     "shell.execute_reply.started": "2022-05-12T09:36:01.62905Z"
    }
   },
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    print('Starting pretraining GMF module')\n",
    "    trainer.fit(gmf,  train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "else:\n",
    "    print('GMF pretraining skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define & Pretrain MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:55:44.073152Z",
     "iopub.status.busy": "2022-05-13T14:55:44.072854Z",
     "iopub.status.idle": "2022-05-13T14:55:59.772349Z",
     "shell.execute_reply": "2022-05-13T14:55:59.771280Z",
     "shell.execute_reply.started": "2022-05-13T14:55:44.073110Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLP(number_of_users, number_of_movies, embedding_size*2,\n",
    "         loss_func=mse_loss, dropout=0.1).to(device)\n",
    "initialize_weights(mlp, 0, 0.01)\n",
    "\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project='cil-project',\n",
    "                           log_model='all', \n",
    "                           name=f'mlp_{time.time():.0f}'\n",
    "                           )\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"mlp/valid_loss\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(callbacks=[checkpoint_callback, lr_monitor], \n",
    "                     gpus=gpus, auto_select_gpus=True,\n",
    "                     max_epochs=num_epochs, \n",
    "                     auto_lr_find=True,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=5,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T14:55:59.774004Z",
     "iopub.status.busy": "2022-05-13T14:55:59.773689Z"
    }
   },
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    print('Starting MLP pretraining.')\n",
    "    trainer.fit(mlp,  train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "else:\n",
    "    print('MLP pretraining passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T10:12:55.205458Z",
     "iopub.status.busy": "2022-05-12T10:12:55.205222Z",
     "iopub.status.idle": "2022-05-12T10:12:55.209341Z",
     "shell.execute_reply": "2022-05-12T10:12:55.208526Z",
     "shell.execute_reply.started": "2022-05-12T10:12:55.205431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment when running for the first time in a scheduler, so that when this notebook fails \n",
    "# go to wandb, check the artifact to use and update \n",
    "# corresponding field in NeuMF layer\n",
    "assert not pretrain, 'Pretrain mode is on, not continuing with NeuMF training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuMF Layer\n",
    "Download pretrained weights and initialize NeuMF layer and train on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T13:09:39.284683Z",
     "iopub.status.busy": "2022-05-13T13:09:39.283803Z",
     "iopub.status.idle": "2022-05-13T13:09:54.374850Z",
     "shell.execute_reply": "2022-05-13T13:09:54.373613Z",
     "shell.execute_reply.started": "2022-05-13T13:09:39.284632Z"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "run = wandb.init(project='cil-project')\n",
    "gmf_artifact = run.use_artifact(f\"{os.environ['WANDB_ENTITY']}/cil-project/model-26mwkhfd:v10\", type='model')\n",
    "mlp_artifact = run.use_artifact(f\"{os.environ['WANDB_ENTITY']}/cil-project/model-9cngz9p3:v5\", type='model')\n",
    "gmf_artifact_dir = gmf_artifact.download()\n",
    "mlp_artifact_dir = mlp_artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T13:09:54.377662Z",
     "iopub.status.busy": "2022-05-13T13:09:54.377168Z",
     "iopub.status.idle": "2022-05-13T13:09:54.831123Z",
     "shell.execute_reply": "2022-05-13T13:09:54.829583Z",
     "shell.execute_reply.started": "2022-05-13T13:09:54.377620Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(gmf_artifact_dir)\n",
    "os.path.abspath(os.path.join(gmf_artifact_dir, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T13:09:54.834277Z",
     "iopub.status.busy": "2022-05-13T13:09:54.833125Z",
     "iopub.status.idle": "2022-05-13T13:09:55.245349Z",
     "shell.execute_reply": "2022-05-13T13:09:55.244592Z",
     "shell.execute_reply.started": "2022-05-13T13:09:54.834218Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuMF(number_of_users, number_of_movies, embedding_size,\n",
    "              gmf_pretrained_ckpt=os.path.abspath(os.path.join(gmf_artifact_dir, 'model.ckpt')),\n",
    "              mlp_pretrained_ckpt=os.path.abspath(os.path.join(mlp_artifact_dir, 'model.ckpt')),\n",
    "             loss_func=mse_loss).to(device)\n",
    "wandb.watch(model)\n",
    "\n",
    "wandb_logger = WandbLogger(project='cil-project',\n",
    "                           log_model='all', \n",
    "                           name=run\n",
    "                           )\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"neumf/valid_loss\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    ")\n",
    "trainer = pl.Trainer(callbacks=[checkpoint_callback, lr_monitor], \n",
    "                     gpus=gpus, auto_select_gpus=True,\n",
    "                     max_epochs=num_epochs, \n",
    "                     auto_lr_find=True,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=5,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T10:15:59.086129Z",
     "iopub.status.busy": "2022-05-12T10:15:59.085535Z",
     "iopub.status.idle": "2022-05-12T10:20:11.661998Z",
     "shell.execute_reply": "2022-05-12T10:20:11.661041Z",
     "shell.execute_reply.started": "2022-05-12T10:15:59.086089Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit\n",
    "## Submission Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pd = pd.read_csv('/kaggle/input/cil-collaborative-filtering-2022/sampleSubmission.csv')\n",
    "submission_users, submission_movies, submission_ratings = extract_users_items_predictions(submission_pd)\n",
    "submission_users_torch = torch.tensor(submission_users, device=device)\n",
    "submission_movies_torch = torch.tensor(submission_movies, device=device)\n",
    "# create submission dataloader\n",
    "submit_loader = DataLoader(TensorDataset(submission_users_torch, submission_movies_torch),\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict entries using trainer\n",
    "submission_predictions = trainer.predict(model, submit_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit\n",
    "Transfer predictions for each batch to the data frame (can use model(batch) then trainer.predict for memory efficiency) and then write it to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_i = 0\n",
    "for pred_batch in submission_predictions:\n",
    "    start_ind = submission_i*batch_size\n",
    "    end_ind = pred_batch.shape[0] + start_ind\n",
    "    preds_cpu = np.array(pred_batch.cpu())\n",
    "    np.clip(preds_cpu, a_min=1, a_max=5, out=preds_cpu)\n",
    "    submission_pd.iloc[start_ind: end_ind, 1] = preds_cpu\n",
    "    submission_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pd.head, submission_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'mlp_embedding2_dropout_submission.csv'\n",
    "submission_pd.to_csv(file_name, encoding='utf-8', index=False)\n",
    "# just in case upload submission to wandb\n",
    "wandb.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "! head *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create download link\n",
    "from IPython.display import FileLink \n",
    "FileLink(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
